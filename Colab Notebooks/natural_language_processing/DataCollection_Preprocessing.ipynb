{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"DataCollection_Preprocessing.ipynb","provenance":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"F7sZMBlbUZh4"},"source":["# Tutorial 1: Collect and Clean Twitter Data"]},{"cell_type":"markdown","metadata":{"id":"SgQFexXZUZiA"},"source":["## Introduction\n","\n","In this tutorial you will:\n","\n","-  Use the Twitter API to collect COVID19-related Twitter data\n","-  Extract the tweet text and any metadata you require\n","-  Perform preprocessing of the text to allow for better application of NLP techniques.\n","\n","### Let's get started!"]},{"cell_type":"markdown","metadata":{"id":"5atsRvdcVKO9"},"source":["# New Section"]},{"cell_type":"markdown","metadata":{"id":"kMe33a2LVLIw"},"source":["# New Section"]},{"cell_type":"markdown","metadata":{"id":"myxBbl-2UZiB"},"source":["To access the Twitter API, we will make use of the Python library **_Tweepy_**. Let's start by importing tweepy and other libraries needed for this tutorial."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3npBolv6UZiC","executionInfo":{"status":"ok","timestamp":1632026053042,"user_tz":-180,"elapsed":1842,"user":{"displayName":"Joseph Kagotho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8fbUfiycBd_JYkkc9MorHtSTSoCt6d0e8347qjA=s64","userId":"07020421671089022077"}},"outputId":"4e403653-656a-4433-c260-5af4f9cf70db"},"source":["# ___Cell no. 1___\n","\n","import tweepy\n","import pandas as pd\n","import csv\n","import re\n","import numpy as np\n","import matplotlib.pyplot as plt\n","plt.style.use('fivethirtyeight')\n","\n","from wordcloud import WordCloud\n","import nltk \n","from nltk.tokenize.toktok import ToktokTokenizer\n","tokenizer = ToktokTokenizer()\n","from nltk.corpus import stopwords\n","nltk.download(\"stopwords\")"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"4ywMNGYuUkeV"},"source":["# New Section"]},{"cell_type":"markdown","metadata":{"id":"WBQv6HyfUZiG"},"source":["<hr>"]},{"cell_type":"markdown","metadata":{"id":"mE48_Ip8UZiG"},"source":["## Section 1: Data Collection\n","\n","In order to access the Twitter API so that you can collect Twitter data, you will need to enter your user credentials into the existing **twitter_credentials.py** file. You can find this file on the Jupyter home page - click on it to edit, then copy and paste in your credentials and save the changes. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":333},"id":"LcNPgKmCUZiH","executionInfo":{"status":"error","timestamp":1632026096113,"user_tz":-180,"elapsed":614,"user":{"displayName":"Joseph Kagotho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8fbUfiycBd_JYkkc9MorHtSTSoCt6d0e8347qjA=s64","userId":"07020421671089022077"}},"outputId":"6d9ef815-6970-49b8-a7ed-46f495db109e"},"source":["# ___Cell no. 2___\n","\n","# We then import the credentials\n","import twitter_credentials as tc"],"execution_count":7,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-ff1c96182e9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# We then import the credentials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtwitter_credentials\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'twitter_credentials'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","metadata":{"id":"HOlUFP1wUZiI"},"source":["Next, we authenticate using our API keys and tokens as follows:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":231},"id":"6hyRanFjUZiJ","executionInfo":{"status":"error","timestamp":1632026092310,"user_tz":-180,"elapsed":1017,"user":{"displayName":"Joseph Kagotho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8fbUfiycBd_JYkkc9MorHtSTSoCt6d0e8347qjA=s64","userId":"07020421671089022077"}},"outputId":"0f7cda12-9cdd-45aa-a9da-c86b98031864"},"source":["# ___Cell no. 3___\n","\n","# Create an authentication object of the AuthHandler class by passing in the credentials\n","auth = tweepy.OAuthHandler(tc.api_key, tc.api_secret_key)\n","\n","# Set the access tokens to complete the authentication process\n","auth.set_access_token(tc.access_token, tc.access_token_secret)"],"execution_count":6,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-e1d7c47e1312>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create an authentication object of the AuthHandler class by passing in the credentials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOAuthHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_secret_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Set the access tokens to complete the authentication process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tc' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"ud48EY1iUZiK"},"source":["We then create the API object while passing in the authentication information"]},{"cell_type":"code","metadata":{"id":"3JpAcwudUZiK","executionInfo":{"status":"aborted","timestamp":1632026092291,"user_tz":-180,"elapsed":994,"user":{"displayName":"Joseph Kagotho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8fbUfiycBd_JYkkc9MorHtSTSoCt6d0e8347qjA=s64","userId":"07020421671089022077"}}},"source":["# ___Cell no. 4___\n","\n","api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NxPI6bS3UZiL"},"source":["In the above line of code we also ensure that if we are rate limited by the Twitter api while submitting queries, we will wait until the rate limit time has lapsed and then continue to collect data."]},{"cell_type":"markdown","metadata":{"id":"719RyinBUZiL"},"source":["We are now ready to build a Standard API search query. You can find more information on search rules and filtering for standard Twitter API searches [here](https://developer.twitter.com/en/docs/tweets/rules-and-filtering/overview/standard-operators)."]},{"cell_type":"code","metadata":{"id":"Ja9_t4m4UZiM","executionInfo":{"status":"aborted","timestamp":1632026092293,"user_tz":-180,"elapsed":19,"user":{"displayName":"Joseph Kagotho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8fbUfiycBd_JYkkc9MorHtSTSoCt6d0e8347qjA=s64","userId":"07020421671089022077"}}},"source":["# ___Cell no. 5___\n","\n","# Enter your search words in accordance with the basic filtering rules\n","\n","l1 = [\"covid or coronavirus\"]\n","l1.append([\"education or elearning\"])\n","search_words = (l1)\n","\n","# We also want to exclude retweets and replies as this may sway results\n","my_search = search_words + \" -filter:retweets\" + \" -filter:replies\"  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_It3Es8PUZiN"},"source":["Below is a simple API search where we filter tweets based on our chosen search words and the language, which is chosen to be English in this case (Twitter supports 34 different languages). The extended tweet mode allows us to load the full text of the tweet which is otherwise truncated."]},{"cell_type":"code","metadata":{"id":"MLKk7l20UZiO","executionInfo":{"status":"aborted","timestamp":1632026092294,"user_tz":-180,"elapsed":20,"user":{"displayName":"Joseph Kagotho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8fbUfiycBd_JYkkc9MorHtSTSoCt6d0e8347qjA=s64","userId":"07020421671089022077"}}},"source":["# ___Cell no. 6___\n","\n","# The Twitter data is stored in a Tweet object which we've called tweets\n","tweets = api.search(q=my_search,lang=\"en\",tweet_mode=\"extended\",count=100)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9kfDrIgoUZiP"},"source":["Now let's print the text of the first 20 tweets to see if our query is working correctly."]},{"cell_type":"code","metadata":{"id":"suV6McEqUZiP","executionInfo":{"status":"aborted","timestamp":1632026092295,"user_tz":-180,"elapsed":21,"user":{"displayName":"Joseph Kagotho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8fbUfiycBd_JYkkc9MorHtSTSoCt6d0e8347qjA=s64","userId":"07020421671089022077"}}},"source":["# ___Cell no. 7___\n","\n","# Iterate and print tweets\n","i = 1\n","for tweet in tweets[0:20]:\n","    print(str(i) + ') ' + tweet.full_text + '\\n')\n","    i = i + 1 "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VUwa1dhlUZiQ"},"source":["Above we have accessed only the text ('full_text') attribute of the Tweet object 'tweets'. You can find the full list of attributes [here](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object), but lets have a quick look at all the data associated with a single tweet by printing the first index of the 'tweets' object."]},{"cell_type":"code","metadata":{"id":"O5VqSi4hUZiR","executionInfo":{"status":"aborted","timestamp":1632026092296,"user_tz":-180,"elapsed":21,"user":{"displayName":"Joseph Kagotho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8fbUfiycBd_JYkkc9MorHtSTSoCt6d0e8347qjA=s64","userId":"07020421671089022077"}}},"source":["# ___Cell no. 8___\n","\n","print(tweets[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"esvXs1zPUZiR"},"source":["As you can see, there is A LOT of information, or, metadata associated with just one tweet. We are able to access all of this information to aid any commercial project or research efforts. The above raw output is in the well-known **JSON** (JavaScript Object Notation) database format."]},{"cell_type":"markdown","metadata":{"id":"GjHMYlGSUZiR"},"source":["The api search method we have used above is, however, limited by the tweet count, for which the maximum number of tweets collected by a single search is limited to 100. An easy solution to this problem is to make use of tweepy's [Cursor object](http://docs.tweepy.org/en/latest/cursor_tutorial.html), which is a pagination tool - i.e. it allows us to page through stored tweets (from up to 7 days prior) and is therefore able to return larger numbers of tweets."]},{"cell_type":"markdown","metadata":{"id":"C0c2BFHaUZiS"},"source":["Next let us submit a query for 1000 tweets and save some of the metadata related to each tweet. Bare in mind that if you try to request more than 1000 tweets per minute from Twitter, you will most likely be rate-limited. Therefore, if you'd like to collect more than 1000 tweets, run the request, save your tweets and wait for at least a minute before rerunning the request. If you do come cross an error while using the Twitter API, take note of the error code and investigate the reason for your error [here](https://developer.twitter.com/en/support/twitter-api/error-troubleshooting)."]},{"cell_type":"code","metadata":{"id":"puIfqtXKUZiS","executionInfo":{"status":"aborted","timestamp":1632026092297,"user_tz":-180,"elapsed":22,"user":{"displayName":"Joseph Kagotho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8fbUfiycBd_JYkkc9MorHtSTSoCt6d0e8347qjA=s64","userId":"07020421671089022077"}}},"source":["# ___Cell no. 9___\n","\n","# Our new method of collecting the tweets\n","tweets = tweepy.Cursor(api.search,q=my_search,lang=\"en\",tweet_mode='extended').items(1000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dN_GPAzSUZiT"},"source":["Below we extract the full text of the tweet, as well as the date and time information and the user location (as entered by the respective user). This process should take about 2 minutes - note the time [magic command](https://ipython.readthedocs.io/en/stable/interactive/magics.html) at the top of the cell which will inform us of the runtime of this particular cell."]},{"cell_type":"code","metadata":{"id":"YVaCGxDcUZiT","executionInfo":{"status":"aborted","timestamp":1632026092298,"user_tz":-180,"elapsed":22,"user":{"displayName":"Joseph Kagotho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8fbUfiycBd_JYkkc9MorHtSTSoCt6d0e8347qjA=s64","userId":"07020421671089022077"}}},"source":["%%time\n","\n","# ___Cell no. 10___\n","\n","# Extract the info we need from the tweets object\n","tweet_info = [[tweet.id_str,tweet.created_at,tweet.user.location,tweet.full_text] for tweet in tweets]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XDj5xARsUZiU"},"source":["Now let's put all this data into a Pandas dataframe!"]},{"cell_type":"code","metadata":{"id":"CohChHnuUZiU","executionInfo":{"status":"aborted","timestamp":1632026092299,"user_tz":-180,"elapsed":23,"user":{"displayName":"Joseph Kagotho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8fbUfiycBd_JYkkc9MorHtSTSoCt6d0e8347qjA=s64","userId":"07020421671089022077"}}},"source":["# ___Cell no. 11___\n","\n","# Put our data into a dataframe \n","df = pd.DataFrame(data=tweet_info, columns=['tweet_id_str','date_time','location','tweet_text'])\n","\n","# Have a quick look at the dataframe\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d3tOjdAoUZiU"},"source":["Now let's have a look at the text from the first 20 tweets."]},{"cell_type":"code","metadata":{"id":"PcePDvy5UZiV","executionInfo":{"status":"aborted","timestamp":1632026092299,"user_tz":-180,"elapsed":23,"user":{"displayName":"Joseph Kagotho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8fbUfiycBd_JYkkc9MorHtSTSoCt6d0e8347qjA=s64","userId":"07020421671089022077"}}},"source":["# ___Cell no. 12___\n","\n","for i,tweet in enumerate(df['tweet_text'].head(20)):\n","    print(i+1, tweet, '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zOrNfCIQUZiV"},"source":["<hr>"]},{"cell_type":"markdown","metadata":{"id":"BUjkJNNmUZiW"},"source":["## Section 2: Data Cleaning and Pre-processing\n","\n","Before we start to analyse our data, we need to perform some cleaning and preprocessing of the text in order to get meaningful results from NLP techniques. The function below will clean the data by removing hyperlinks, special characters, emojis and @mentions from the tweets. This is achieved by using the [Regular Expressions](https://www.tutorialspoint.com/python/python_reg_expressions.htm) module **re** to serach for an expression (r'expression') and replace it with an empty string using the method **sub**."]},{"cell_type":"code","metadata":{"id":"L9kNppntUZiW","executionInfo":{"status":"aborted","timestamp":1632026092300,"user_tz":-180,"elapsed":23,"user":{"displayName":"Joseph Kagotho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8fbUfiycBd_JYkkc9MorHtSTSoCt6d0e8347qjA=s64","userId":"07020421671089022077"}}},"source":["# ___Cell no. 13___\n","\n","def clean_text(text):\n","    \n","    \"\"\"\n","    A function to clean the tweet text\n","    \"\"\"\n","    #Remove hyper links\n","    text = re.sub(r'https?:\\/\\/\\S+', ' ', text)\n","    \n","    #Remove @mentions\n","    text = re.sub(r'@[A-Za-z0-9]+', ' ', text)\n","    \n","    #Remove anything that isn't a letter, number, or one of the punctuation marks listed\n","    text = re.sub(r\"[^A-Za-z0-9#'?!,.]+\", ' ', text)   \n","    \n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C21diKTjUZiX","executionInfo":{"status":"aborted","timestamp":1632026092301,"user_tz":-180,"elapsed":24,"user":{"displayName":"Joseph Kagotho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8fbUfiycBd_JYkkc9MorHtSTSoCt6d0e8347qjA=s64","userId":"07020421671089022077"}}},"source":["# ___Cell no. 14___\n","\n","# Apply the clean_text function to the 'tweet_text' column\n","df['tweet_text']=df['tweet_text'].apply(clean_text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i-UoDY9-UZiX"},"source":["Now let's print the text from the first 20 tweets once again to have a look at the changes after cleaning."]},{"cell_type":"code","metadata":{"id":"VdHo8gDGUZiX","executionInfo":{"status":"aborted","timestamp":1632026092302,"user_tz":-180,"elapsed":25,"user":{"displayName":"Joseph Kagotho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8fbUfiycBd_JYkkc9MorHtSTSoCt6d0e8347qjA=s64","userId":"07020421671089022077"}}},"source":["# ___Cell no. 15___\n","\n","for i,tweet in enumerate(df['tweet_text'].head(20)):\n","    print(i+1, tweet, '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xclv-qzSUZiY"},"source":["You will notice that hyperlinks, spaces and other special characters have been removed. We have left in the '#' symbol due to it's relevance in tweets."]},{"cell_type":"markdown","metadata":{"id":"TaK3_cGUUZiY"},"source":["We then convert all the tweet texts to lower case. This is done so that words that exist in both their lower case or upper case forms (or indeed a mixture of the two) in our text are not processed as two different words."]},{"cell_type":"code","metadata":{"id":"nAb3hYbcUZiZ","executionInfo":{"status":"aborted","timestamp":1632026092302,"user_tz":-180,"elapsed":24,"user":{"displayName":"Joseph Kagotho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8fbUfiycBd_JYkkc9MorHtSTSoCt6d0e8347qjA=s64","userId":"07020421671089022077"}}},"source":["# ___Cell no. 16___\n","\n","df['tweet_text']=df['tweet_text'].str.lower()\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"36BYJwL_UZia"},"source":["Next, we remove all _stop words_ from our text, these are **commonly used** words such as “the”, “a”, “if”, “in”, etc. which do not contribute to our NLP objectives, i.e. these stop words do not provide any information about the sentiment of the text. We therefore would not want these words to take up space in our database, or to consume valuable processing time.\n","\n","The tweet texts will look quite strange once stop words are removed. One may then wonder why we would want to do this, as the resulting tweets do not make much sense when we read them. The need for stop word removal will be better undersood in Tutorial 3.\n","\n","The Python NLTK (Natural Language Toolkit) has a list of stop words stored in 16 different languages. Below, we make use of the NLTK catalogue of english stop words."]},{"cell_type":"code","metadata":{"id":"m75zqnVKUZib","executionInfo":{"status":"aborted","timestamp":1632026092303,"user_tz":-180,"elapsed":25,"user":{"displayName":"Joseph Kagotho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8fbUfiycBd_JYkkc9MorHtSTSoCt6d0e8347qjA=s64","userId":"07020421671089022077"}}},"source":["# ___Cell no. 17___\n","\n","# Get the list of NLTK stop words\n","\n","stopwords = stopwords.words(\"english\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cLKQzjrsUZic","executionInfo":{"status":"aborted","timestamp":1632026092304,"user_tz":-180,"elapsed":26,"user":{"displayName":"Joseph Kagotho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8fbUfiycBd_JYkkc9MorHtSTSoCt6d0e8347qjA=s64","userId":"07020421671089022077"}}},"source":["# ___Cell no. 18___\n","\n","# Let's have a quick look at what words nltk considers to be stop words\n","stopwords"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oTtdRrbqUZic"},"source":["We can also extend the NLTK stopwords list by adding stopwords of our own. For example, we would want to add the words that we used in our search query, as we are guaranteed that at least one of these words will occur in each tweet and should thus be excluded from our corpus (In NLP, the word _corpus_ refers to a collection of texts to be analysed)."]},{"cell_type":"code","metadata":{"id":"_FWOfYSuUZic","executionInfo":{"status":"aborted","timestamp":1632026092305,"user_tz":-180,"elapsed":26,"user":{"displayName":"Joseph Kagotho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8fbUfiycBd_JYkkc9MorHtSTSoCt6d0e8347qjA=s64","userId":"07020421671089022077"}}},"source":["# ___Cell no. 19___\n","\n","# Define our own list of stopwords\n","my_stopwords = ['coronavirus','covid','pandemic','covid19','lockdown','amp','via']\n","\n","# Extend the nltk stopwords list\n","stopwords.extend(my_stopwords)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hr01cj9cUZid","executionInfo":{"status":"aborted","timestamp":1632026092305,"user_tz":-180,"elapsed":26,"user":{"displayName":"Joseph Kagotho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8fbUfiycBd_JYkkc9MorHtSTSoCt6d0e8347qjA=s64","userId":"07020421671089022077"}}},"source":["# ___Cell no. 20___\n","\n","def remove_stopwords(text):\n","    \n","    \"\"\"\n","    A function to remove stop words\n","    \"\"\"\n","    \n","    # Tokenize the text\n","    tokens = tokenizer.tokenize(text)\n","    tokens = [token.strip() for token in tokens]\n","    \n","    # Remove stop words\n","    filtered_tokens = [token for token in tokens if token not in stopwords]\n","    filtered_text = ' '.join(filtered_tokens)    \n","    \n","    return filtered_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HUW8F9QoUZid","executionInfo":{"status":"aborted","timestamp":1632026092306,"user_tz":-180,"elapsed":27,"user":{"displayName":"Joseph Kagotho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8fbUfiycBd_JYkkc9MorHtSTSoCt6d0e8347qjA=s64","userId":"07020421671089022077"}}},"source":["# ___Cell no. 21___\n","\n","# Apply the stopword removal function to the text of all tweets\n","df['tweet_text']=df['tweet_text'].apply(remove_stopwords)\n","\n","# Print the first 20 tweets\n","for i,tweet in enumerate(df['tweet_text'].head(20)):\n","    print(i+1, tweet, '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HJ4bpXhUUZie"},"source":["Now that we've cleaned and preprocessed our tweet text, let's get a quick idea of the common words that appear in the tweets in the form of a Word Cloud!"]},{"cell_type":"code","metadata":{"id":"RdMvDjBGUZie","executionInfo":{"status":"aborted","timestamp":1632026092307,"user_tz":-180,"elapsed":28,"user":{"displayName":"Joseph Kagotho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8fbUfiycBd_JYkkc9MorHtSTSoCt6d0e8347qjA=s64","userId":"07020421671089022077"}}},"source":["# ___Cell no. 22___\n","\n","# Plot a word cloud\n","\n","all_words = ' '.join( [data for data in df['tweet_text']])\n","word_cloud = WordCloud(width=300, height=200, random_state=21, max_font_size = 300,\n","                       stopwords=stopwords).generate(all_words)\n","\n","plt.figure(figsize = (20,10))\n","plt.imshow(word_cloud, interpolation='bilinear')\n","plt.axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sqmVIlBuUZif"},"source":["<hr>"]},{"cell_type":"markdown","metadata":{"id":"OF7xCIExUZif"},"source":["## Section 3: Refining your Twitter API search query"]},{"cell_type":"markdown","metadata":{"id":"QYfMPqHfUZif"},"source":["We can also further refine a Twitter API search to include the dates from which we would like to collect tweets, as well as the location. The **__geocode__** parameter allows you to define the latitude and longitude of a location, as well a \"search radius\" - you will also specify the radius units, i.e. \"km\"."]},{"cell_type":"markdown","metadata":{"id":"LI7b2PeqUZig"},"source":["<div class=\"alert alert-block alert-info\">\n","    \n","<b>Note:</b> The standard Twitter API only allows you to access tweets from the last 7 days, so be sure to adjust the dates in the code accordingly.\n","\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"W5FS_0dHUZig"},"source":["![Screenshot%20from%202020-07-27%2022-55-51.png](attachment:Screenshot%20from%202020-07-27%2022-55-51.png)"]},{"cell_type":"code","metadata":{"id":"BS2LANm_UZih","executionInfo":{"status":"aborted","timestamp":1632026092308,"user_tz":-180,"elapsed":28,"user":{"displayName":"Joseph Kagotho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8fbUfiycBd_JYkkc9MorHtSTSoCt6d0e8347qjA=s64","userId":"07020421671089022077"}}},"source":["# ___Cell no. 23___\n","\n","search_words = \"coronavirus OR covid OR pandemic OR covid19 OR lockdown\"\n","\n","# Latitude, logitude and search radius(km) for Zambia and some surrounding regions\n","loc = \"0.0136,40.9062,500km\"\n","\n","# Search dates\n","date_since = \"2021-07-24\"\n","date_until = \"2021-07-26\"\n","\n","# We also want to exclude retweets and replies as this may sway results\n","my_search = search_words + \" -filter:retweets\" + \" -filter:replies\"  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4SR4DxxLUZih","executionInfo":{"status":"aborted","timestamp":1632026092308,"user_tz":-180,"elapsed":28,"user":{"displayName":"Joseph Kagotho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8fbUfiycBd_JYkkc9MorHtSTSoCt6d0e8347qjA=s64","userId":"07020421671089022077"}}},"source":["# ___Cell no. 24___\n","\n","# Use the tweepy Cursor method to access tweets from a specified region and between certain dates\n","tweets = tweepy.Cursor(api.search,\n","                       q=my_search,\n","                       lang=\"en\",\n","                       tweet_mode='extended',\n","                       geocode=loc,\n","                       since=date_since,\n","                       until=date_until).items(1000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eC3Thu5lUZii","executionInfo":{"status":"aborted","timestamp":1632026092309,"user_tz":-180,"elapsed":29,"user":{"displayName":"Joseph Kagotho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8fbUfiycBd_JYkkc9MorHtSTSoCt6d0e8347qjA=s64","userId":"07020421671089022077"}}},"source":["%%time\n","\n","# ___Cell no. 25___\n","\n","# Once again, this should take ~2 minutes to run if you are collecting 1000 tweets\n","tweet_info = [[tweet.id_str,tweet.created_at,tweet.user.location,tweet.full_text] for tweet in tweets]\n","\n","# Put our data into a dataframe \n","df_new = pd.DataFrame(data=tweet_info, columns=['tweet_id_str','date_time','location','tweet_text'])\n","\n","# Have a quick look at the dataframe\n","#df_new\n","\n","df = df_new.loc[df_new[\"location\"] == 'Kenya']\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4zz3-q1DUZii"},"source":["We see that the search region specified in our query is correct and that location information is now available for each tweet."]},{"cell_type":"markdown","metadata":{"id":"Tp2jLbUNUZij"},"source":["**Additional challenge**: Try out a few queries of your own! Perhaps search for tweets around topics you're interested in and see what comes up. Feel free to collect tweets for a specific country/region as well. "]},{"cell_type":"markdown","metadata":{"id":"0or1ChW5UZij"},"source":["<div class=\"alert alert-block alert-info\">\n","    \n","<b>Note:</b> Do NOT excdeed 1000 tweets per query, as it is likely that you will then be rate-limited by the Twitter API. However, you are able to run multiple queries.\n","\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"HI1rY7scUZij"},"source":["After completing this tutorial you should be able to:\n","\n","- Collect tweets related to a given topic / search words\n","- Access the respective metadata for each tweet\n","- Collect tweets from any region in the world\n","- Clean text\n","- Convert text to lower case\n","- Remove stop words from the text"]},{"cell_type":"markdown","metadata":{"id":"OcRJh-DdUZik"},"source":["### Tutorial 1 complete! Well done! "]}]}